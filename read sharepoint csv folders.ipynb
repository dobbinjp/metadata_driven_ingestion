{"cells":[{"cell_type":"code","source":["from pyspark.sql.functions import col, input_file_name, current_timestamp, regexp_extract\n","import re"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"22775446-3136-46d7-affe-7d1adc02af8d","normalized_state":"finished","queued_time":"2025-12-18T21:17:23.3050189Z","session_start_time":null,"execution_start_time":"2025-12-18T21:17:23.3062178Z","execution_finish_time":"2025-12-18T21:17:23.600095Z","parent_msg_id":"f8c86c82-b0ef-4d58-8417-8fe860026c6a"},"text/plain":"StatementMeta(, 22775446-3136-46d7-affe-7d1adc02af8d, 8, Finished, Available, Finished)"},"metadata":{}}],"execution_count":6,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8bf93c7c-66db-4769-adbb-161ef27410ec"},{"cell_type":"code","source":["# parameter cell to store csv folder location as well as full table name\n","topfolder = \"Files/jd_sharepoint/MLB\"\n","dbschema = \"dbo\""],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"available","session_id":"22775446-3136-46d7-affe-7d1adc02af8d","normalized_state":"finished","queued_time":"2025-12-18T21:17:23.3647176Z","session_start_time":null,"execution_start_time":"2025-12-18T21:17:23.6022974Z","execution_finish_time":"2025-12-18T21:17:23.9353537Z","parent_msg_id":"113f1eb5-2dcb-4b94-84ce-8712a279f16e"},"text/plain":"StatementMeta(, 22775446-3136-46d7-affe-7d1adc02af8d, 9, Finished, Available, Finished)"},"metadata":{}}],"execution_count":7,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"17ce8401-a844-4fac-bdef-e8511b3bf005"},{"cell_type":"code","source":["# get list of folders in topfolder\n","foldernames = notebookutils.fs.ls(topfolder)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":10,"statement_ids":[10],"state":"finished","livy_statement_state":"available","session_id":"22775446-3136-46d7-affe-7d1adc02af8d","normalized_state":"finished","queued_time":"2025-12-18T21:17:23.460653Z","session_start_time":null,"execution_start_time":"2025-12-18T21:17:23.9374319Z","execution_finish_time":"2025-12-18T21:17:24.8254229Z","parent_msg_id":"404a9b55-9229-4fa8-be95-9865eae6ca68"},"text/plain":"StatementMeta(, 22775446-3136-46d7-affe-7d1adc02af8d, 10, Finished, Available, Finished)"},"metadata":{}}],"execution_count":8,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c1773bea-046f-4fa5-8475-c804c0962525"},{"cell_type":"code","source":["# clean the column names (replace spaces with underscores) using this function...special chars will break parquet columns\n","def clean_column_name(name):\n","    return re.sub(r'[^a-zA-Z0-True0-9]', '_', name)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":11,"statement_ids":[11],"state":"finished","livy_statement_state":"available","session_id":"22775446-3136-46d7-affe-7d1adc02af8d","normalized_state":"finished","queued_time":"2025-12-18T21:17:23.5876004Z","session_start_time":null,"execution_start_time":"2025-12-18T21:17:24.8275381Z","execution_finish_time":"2025-12-18T21:17:25.1259424Z","parent_msg_id":"d7e51f55-bc4a-4f64-9202-6bc3ff604f01"},"text/plain":"StatementMeta(, 22775446-3136-46d7-affe-7d1adc02af8d, 11, Finished, Available, Finished)"},"metadata":{}}],"execution_count":9,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"de239f57-b474-4b06-a44b-8dd418f5099e"},{"cell_type":"code","source":["for f in foldernames:\n","    tablename = dbschema + '.' + clean_column_name(f.name)\n","    tablepath = f.path\n","    print(f\"Now processing {tablename}...\")\n","    \n","    # drop the table if it already exists\n","    spark.sql(f\"drop table if exists {tablename};\")\n","    spark.sql(f\"drop table if exists {tablename}_bad;\")\n","\n","    # read data from csv folder, infer the schema, be permissive and add a field indicating whether the data is corrupted\n","    df = spark.read.format(\"csv\") \\\n","        .option(\"mode\", \"PERMISSIVE\") \\\n","        .option(\"header\", \"true\") \\\n","        .option(\"inferSchema\", \"true\") \\\n","        .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\") \\\n","        .option(\"sep\", \",\") \\\n","        .option(\"quote\", \"\\\"\") \\\n","        .option(\"escape\", \"\\\\\") \\\n","        .option(\"multiLine\", \"true\") \\\n","        .option(\"ignoreLeadingWhiteSpace\", \"true\") \\\n","        .option(\"ignoreTrailingWhiteSpace\", \"true\") \\\n","        .load(tablepath)\n","    \n","    # apply the cleaning function to all columns\n","    new_column_names = [clean_column_name(c) for c in df.columns]\n","    df = df.toDF(*new_column_names)\n","\n","    # add metadata columns\n","    df = df.withColumn(\"_date_ingested_utc\", current_timestamp()) \\\n","        .withColumn(\"source_file\", input_file_name()) \\\n","        .withColumn(\"_source_file\", regexp_extract(col(\"source_file\"), r\"(Files\\/.*\\.csv)\", 1)) \\\n","        .drop(\"source_file\")\n","\n","    # safely handle corrupt rows only if the column exists\n","    if \"_corrupt_record\" in df.columns:\n","        bad_df = df.filter(col(\"_corrupt_record\").isNotNull())\n","        bad_count = bad_df.count()\n","        print(f\"Corrupt rows found: {bad_count}\")\n","        bad_df.show(20, truncate=False)\n","        bad_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{tablename}_bad\")\n","    else:\n","        print(\"     No malformed rows were captured (or none occurred)\")\n","\n","\n","    # write df to a table using overwrite (we will overwrite each time as new data is added to the sharepoint folder)\n","    (\n","        df\n","        .write.format(\"delta\")\n","        .mode(\"overwrite\")\n","        .saveAsTable(tablename)\n","    )\n","\n","    print(f\"...{tablename} processed successfully!\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":12,"statement_ids":[12],"state":"submitted","livy_statement_state":"running","session_id":"22775446-3136-46d7-affe-7d1adc02af8d","normalized_state":"running","queued_time":"2025-12-18T21:17:23.6948094Z","session_start_time":null,"execution_start_time":"2025-12-18T21:17:25.1279855Z","execution_finish_time":null,"parent_msg_id":"66565005-4e6a-4bf5-b2a7-e45aa7928529"},"text/plain":"StatementMeta(, 22775446-3136-46d7-affe-7d1adc02af8d, 12, Submitted, Running, Running)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Now processing dbo.allstarfull...\n     No malformed rows were captured (or none occurred)\n...dbo.allstarfull processed successfully!\nNow processing dbo.batting...\n"]}],"execution_count":10,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"62f5ee0b-cba3-4481-8300-ab71b7a61fc7"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"aa75fe4f-0558-4596-aa46-1e2ac01951c1","known_lakehouses":[{"id":"aa75fe4f-0558-4596-aa46-1e2ac01951c1"}],"default_lakehouse_name":"lh_dayforce","default_lakehouse_workspace_id":"7e47420a-81c2-466a-b53b-6290f6921b4d"}}},"nbformat":4,"nbformat_minor":5}
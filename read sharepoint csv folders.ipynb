{"cells":[{"cell_type":"code","source":["from pyspark.sql.functions import col, input_file_name, current_timestamp, regexp_extract\n","import re"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":65,"statement_ids":[65],"state":"finished","livy_statement_state":"available","session_id":"3c003e81-058f-46a9-8acb-dbb089acfbc7","normalized_state":"finished","queued_time":"2025-12-18T17:00:23.0962292Z","session_start_time":null,"execution_start_time":"2025-12-18T17:00:23.0972431Z","execution_finish_time":"2025-12-18T17:00:24.7958804Z","parent_msg_id":"220c3d20-23bd-48e9-97e4-5296398046ab"},"text/plain":"StatementMeta(, 3c003e81-058f-46a9-8acb-dbb089acfbc7, 65, Finished, Available, Finished)"},"metadata":{}}],"execution_count":63,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8bf93c7c-66db-4769-adbb-161ef27410ec"},{"cell_type":"code","source":["# parameter cell to store csv folder location as well as full table name\n","topfolder = \"Files/jd_sharepoint/MLB\"\n","dbschema = \"dbo\""],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":66,"statement_ids":[66],"state":"finished","livy_statement_state":"available","session_id":"3c003e81-058f-46a9-8acb-dbb089acfbc7","normalized_state":"finished","queued_time":"2025-12-18T17:00:23.1606677Z","session_start_time":null,"execution_start_time":"2025-12-18T17:00:24.7977845Z","execution_finish_time":"2025-12-18T17:00:25.1276715Z","parent_msg_id":"ca9b9f72-f941-47d6-a59b-fef023661b31"},"text/plain":"StatementMeta(, 3c003e81-058f-46a9-8acb-dbb089acfbc7, 66, Finished, Available, Finished)"},"metadata":{}}],"execution_count":64,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"17ce8401-a844-4fac-bdef-e8511b3bf005"},{"cell_type":"code","source":["# get list of folders in topfolder\n","foldernames = notebookutils.fs.ls(topfolder)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":67,"statement_ids":[67],"state":"finished","livy_statement_state":"available","session_id":"3c003e81-058f-46a9-8acb-dbb089acfbc7","normalized_state":"finished","queued_time":"2025-12-18T17:00:23.290711Z","session_start_time":null,"execution_start_time":"2025-12-18T17:00:25.1297099Z","execution_finish_time":"2025-12-18T17:00:26.70233Z","parent_msg_id":"be7668f8-f398-4ce7-b45c-c4666c38001c"},"text/plain":"StatementMeta(, 3c003e81-058f-46a9-8acb-dbb089acfbc7, 67, Finished, Available, Finished)"},"metadata":{}}],"execution_count":65,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c1773bea-046f-4fa5-8475-c804c0962525"},{"cell_type":"code","source":["# clean the column names (replace spaces with underscores) using this function...special chars will break parquet columns\n","def clean_column_name(name):\n","    return re.sub(r'[^a-zA-Z0-True0-9]', '_', name)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":68,"statement_ids":[68],"state":"finished","livy_statement_state":"available","session_id":"3c003e81-058f-46a9-8acb-dbb089acfbc7","normalized_state":"finished","queued_time":"2025-12-18T17:00:23.4095654Z","session_start_time":null,"execution_start_time":"2025-12-18T17:00:26.7046641Z","execution_finish_time":"2025-12-18T17:00:27.0229617Z","parent_msg_id":"a0c22b09-a458-48bc-82da-155afc6ed323"},"text/plain":"StatementMeta(, 3c003e81-058f-46a9-8acb-dbb089acfbc7, 68, Finished, Available, Finished)"},"metadata":{}}],"execution_count":66,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"de239f57-b474-4b06-a44b-8dd418f5099e"},{"cell_type":"code","source":["for f in foldernames:\n","    tablename = dbschema + '.' + clean_column_name(f.name)\n","    tablepath = f.path\n","    print(f\"Now processing {tablename}...\")\n","    \n","    # drop the table if it already exists\n","    spark.sql(f\"drop table if exists {tablename};\")\n","    spark.sql(f\"drop table if exists {tablename}_bad;\")\n","\n","    # read data from csv folder, infer the schema, be permissive and add a field indicating whether the data is corrupted\n","    df = spark.read.format(\"csv\") \\\n","        .option(\"mode\", \"PERMISSIVE\") \\\n","        .option(\"header\", \"true\") \\\n","        .option(\"inferSchema\", \"true\") \\\n","        .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\") \\\n","        .load(tablepath)\n","    \n","    # apply the cleaning function to all columns\n","    new_column_names = [clean_column_name(c) for c in df.columns]\n","    df = df.toDF(*new_column_names)\n","\n","    # add metadata columns\n","    df = df.withColumn(\"_date_ingested_utc\", current_timestamp()) \\\n","        .withColumn(\"source_file\", input_file_name()) \\\n","        .withColumn(\"_source_file\", regexp_extract(col(\"source_file\"), r\"(Files\\/.*\\.csv)\", 1)) \\\n","        .drop(\"source_file\")\n","\n","    # safely handle corrupt rows only if the column exists\n","    if \"_corrupt_record\" in df.columns:\n","        bad_df = df.filter(col(\"_corrupt_record\").isNotNull())\n","        bad_count = bad_df.count()\n","        print(f\"Corrupt rows found: {bad_count}\")\n","        bad_df.show(20, truncate=False)\n","        bad_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{tablename}_bad\")\n","    else:\n","        print(\"     No malformed rows were captured (or none occurred)\")\n","\n","\n","    # write df to a table using overwrite (we will overwrite each time as new data is added to the sharepoint folder)\n","    (\n","        df\n","        .write.format(\"delta\")\n","        .mode(\"overwrite\")\n","        .saveAsTable(tablename)\n","    )\n","\n","    print(f\"...{tablename} processed successfully!\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":69,"statement_ids":[69],"state":"finished","livy_statement_state":"available","session_id":"3c003e81-058f-46a9-8acb-dbb089acfbc7","normalized_state":"finished","queued_time":"2025-12-18T17:00:23.4715689Z","session_start_time":null,"execution_start_time":"2025-12-18T17:00:27.0249664Z","execution_finish_time":"2025-12-18T17:01:26.1801174Z","parent_msg_id":"c32e1e02-1004-4df5-b4cb-4dcdcc0641b6"},"text/plain":"StatementMeta(, 3c003e81-058f-46a9-8acb-dbb089acfbc7, 69, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Now processing dbo.allstarfull...\n     No malformed rows were captured (or none occurred).\n...dbo.allstarfull processed successfully!\nNow processing dbo.batting...\n     No malformed rows were captured (or none occurred).\n...dbo.batting processed successfully!\nNow processing dbo.fielding...\n     No malformed rows were captured (or none occurred).\n...dbo.fielding processed successfully!\nNow processing dbo.homegames...\n     No malformed rows were captured (or none occurred).\n...dbo.homegames processed successfully!\nNow processing dbo.parks...\n     No malformed rows were captured (or none occurred).\n...dbo.parks processed successfully!\nNow processing dbo.people...\n     No malformed rows were captured (or none occurred).\n...dbo.people processed successfully!\nNow processing dbo.teams...\n     No malformed rows were captured (or none occurred).\n...dbo.teams processed successfully!\n"]}],"execution_count":67,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"62f5ee0b-cba3-4481-8300-ab71b7a61fc7"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"aa75fe4f-0558-4596-aa46-1e2ac01951c1","known_lakehouses":[{"id":"aa75fe4f-0558-4596-aa46-1e2ac01951c1"}],"default_lakehouse_name":"lh_dayforce","default_lakehouse_workspace_id":"7e47420a-81c2-466a-b53b-6290f6921b4d"}}},"nbformat":4,"nbformat_minor":5}